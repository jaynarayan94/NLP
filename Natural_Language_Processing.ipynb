{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Natural Language Processing.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jaynarayan94/NLP/blob/master/Natural_Language_Processing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dzpBKRTHix3C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import the regex module\n",
        "import re\n",
        "\n",
        "# Write a pattern to match sentence endings: sentence_endings\n",
        "sentence_endings = r\"[.?!]\"\n",
        "\n",
        "# Split my_string on sentence endings and print the result\n",
        "print(re.split(sentence_endings, my_string))\n",
        "\n",
        "# Find all capitalized words in my_string and print the result\n",
        "capitalized_words = r\"[A-Z]\\w+\"\n",
        "print(re.findall(capitalized_words, my_string))\n",
        "\n",
        "# Split my_string on spaces and print the result\n",
        "spaces = r\"\\s+\"\n",
        "print(re.split(spaces, my_string))\n",
        "\n",
        "# Find all digits in my_string and print the result\n",
        "digits = r\"\\d+\"\n",
        "print(re.findall(digits, my_string))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NvdwDailk6Cy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import necessary modules\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "# Split scene_one into sentences: sentences\n",
        "sentences = sent_tokenize(scene_one)\n",
        "\n",
        "# Use word_tokenize to tokenize the fourth sentence: tokenized_sent\n",
        "tokenized_sent = word_tokenize(sentences[3])\n",
        "\n",
        "# Make a set of unique tokens in the entire scene: unique_tokens\n",
        "unique_tokens = set(word_tokenize(scene_one))\n",
        "\n",
        "# Print the unique tokens result\n",
        "print(unique_tokens)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lk15Ox5Kk6GS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Search for the first occurrence of \"coconuts\" in scene_one: match\n",
        "match = re.search(\"coconuts\", scene_one)\n",
        "\n",
        "# Print the start and end indexes of match\n",
        "print(match.start(), match.end())\n",
        "\n",
        "# Write a regular expression to search for anything in square brackets: pattern1\n",
        "pattern1 = r\"\\[.*]\"\n",
        "\n",
        "# Use re.search to find the first text in square brackets\n",
        "print(re.search(pattern1, scene_one))\n",
        "\n",
        "# Find the script notation at the beginning of the fourth sentence and print it\n",
        "pattern2 = r\"[\\w\\s]+:\"\n",
        "print(re.match(pattern2, sentences[3]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLuMXV6JuGHU",
        "colab_type": "text"
      },
      "source": [
        "## Regex with NLTK tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CPXFh40lk6Jb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import the necessary modules\n",
        "from nltk.tokenize import regexp_tokenize\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "\n",
        "# Define a regex pattern to find hashtags: pattern1\n",
        "pattern1 = r\"#\\w+\"\n",
        "\n",
        "# Use the pattern on the first tweet in the tweets list\n",
        "regexp_tokenize(tweets[0], pattern1)\n",
        "\n",
        "# Write a pattern that matches both mentions and hashtags\n",
        "pattern2 = r\"([@#]\\w+)\"\n",
        "\n",
        "# Use the pattern on the last tweet in the tweets list\n",
        "regexp_tokenize(tweets[-1], pattern2)\n",
        "\n",
        "# Use the TweetTokenizer to tokenize all tweets into one list\n",
        "tknzr = TweetTokenizer()\n",
        "all_tokens = [tknzr.tokenize(t) for t in tweets]\n",
        "print(all_tokens)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YufwOCLfk6MP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Tokenize and print all words in german_text\n",
        "all_words = word_tokenize(german_text)\n",
        "print(all_words)\n",
        "\n",
        "# Tokenize and print only capital words\n",
        "capital_words = r\"[A-ZÃœ]\\w+\"\n",
        "print(regexp_tokenize(german_text, capital_words))\n",
        "\n",
        "# Tokenize and print only emoji\n",
        "emoji = \"['\\U0001F300-\\U0001F5FF'|'\\U0001F600-\\U0001F64F'|'\\U0001F680-\\U0001F6FF'|'\\u2600-\\u26FF\\u2700-\\u27BF']\"\n",
        "print(regexp_tokenize(german_text, emoji))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HbFPR3hIk6Oq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Split the script into lines: lines\n",
        "lines = holy_grail.split('\\n')\n",
        "\n",
        "# Replace all script lines for speaker\n",
        "pattern = \"[A-Z]{2,}(\\s)?(#\\d)?([A-Z]{2,})?:\"\n",
        "lines = [re.sub(pattern, '', l) for l in lines]\n",
        "\n",
        "# Tokenize each line: tokenized_lines\n",
        "tokenized_lines = [regexp_tokenize(s, \"\\w+\") for s in lines]\n",
        "\n",
        "# Make a frequency list of lengths: line_num_words\n",
        "line_num_words = [len(t_line) for t_line in tokenized_lines]\n",
        "\n",
        "# Plot a histogram of the line lengths\n",
        "plt.hist(line_num_words)\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mVfo0Qi01WbM",
        "colab_type": "text"
      },
      "source": [
        "## Building a Counter with bag-of-words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zGYVMxUpk6WQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import Counter\n",
        "from collections import Counter\n",
        "\n",
        "# Tokenize the article: tokens\n",
        "tokens = word_tokenize(article)\n",
        "\n",
        "# Convert the tokens into lowercase: lower_tokens\n",
        "lower_tokens = [t.lower() for t in tokens]\n",
        "\n",
        "# Create a Counter with the lowercase tokens: bow_simple\n",
        "bow_simple = Counter(lower_tokens)\n",
        "\n",
        "# Print the 10 most common tokens\n",
        "print(bow_simple.most_common(10))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IOcCBExX2q3U",
        "colab_type": "text"
      },
      "source": [
        "## Text preprocessing practice"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RIAjg1ULk6Zf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import WordNetLemmatizer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Retain alphabetic words: alpha_only\n",
        "alpha_only = [t for t in lower_tokens if t.isalpha()]\n",
        "\n",
        "# Remove all stop words: no_stops\n",
        "no_stops = [t for t in alpha_only if t not in english_stops]\n",
        "\n",
        "# Instantiate the WordNetLemmatizer\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Lemmatize all tokens into a new list: lemmatized\n",
        "lemmatized = [wordnet_lemmatizer.lemmatize(t) for t in no_stops]\n",
        "\n",
        "# Create the bag-of-words: bow\n",
        "bow = Counter(lemmatized)\n",
        "\n",
        "# Print the 10 most common tokens\n",
        "print(bow.most_common(10))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IRyD0Wvh6XNg",
        "colab_type": "text"
      },
      "source": [
        "## Word vectors \n",
        "Word vectors are multi-dimensional mathematical representations of words created using deep learning methods. They give us insight into relationships between words in a corpus.\n",
        "\n",
        "## Creating and querying a corpus with gensim"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79V9EKeCk6iA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import Dictionary\n",
        "from gensim.corpora.dictionary import Dictionary\n",
        "\n",
        "# Create a Dictionary from the articles: dictionary\n",
        "dictionary = Dictionary(articles)\n",
        "\n",
        "# Select the id for \"computer\": computer_id\n",
        "computer_id = dictionary.token2id.get(\"computer\")\n",
        "\n",
        "# Use computer_id with the dictionary to print the word\n",
        "print(dictionary.get(computer_id))\n",
        "\n",
        "# Create a MmCorpus: corpus\n",
        "corpus = [dictionary.doc2bow(article) for article in articles]\n",
        "\n",
        "# Print the first 10 word ids with their frequency counts from the fifth document\n",
        "print(corpus[4][:10])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJ4iMjZ77axG",
        "colab_type": "text"
      },
      "source": [
        "### Gensim bag-of-words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qIyJb3OFk6lh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Save the fifth document: doc\n",
        "doc = corpus[4]\n",
        "\n",
        "# Sort the doc for frequency: bow_doc\n",
        "bow_doc = sorted(doc, key=lambda w: w[1], reverse=True)\n",
        "\n",
        "# Print the top 5 words of the document alongside the count\n",
        "for word_id, word_count in bow_doc[:5]:\n",
        "    print(dictionary.get(word_id), word_count)\n",
        "    \n",
        "# Create the defaultdict: total_word_count\n",
        "total_word_count = defaultdict(int)\n",
        "for word_id, word_count in itertools.chain.from_iterable(corpus):\n",
        "    total_word_count[word_id] += word_count\n",
        "\n",
        "# Create a sorted list from the defaultdict: sorted_word_count \n",
        "sorted_word_count = sorted(total_word_count.items(), key=lambda w: w[1], reverse=True) \n",
        "\n",
        "# Print the top 5 words across all documents alongside the count\n",
        "for word_id, word_count in sorted_word_count[:5]:\n",
        "    print(dictionary.get(word_id), word_count)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GlFRzCRd-mjm",
        "colab_type": "text"
      },
      "source": [
        "## Tf-idf with Wikipedia"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vxCN2lGDk6gM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import TfidfModel\n",
        "from gensim.models.tfidfmodel import TfidfModel\n",
        "\n",
        "# Create a new TfidfModel using the corpus: tfidf\n",
        "tfidf = TfidfModel(corpus)\n",
        "\n",
        "# Calculate the tfidf weights of doc: tfidf_weights\n",
        "tfidf_weights = tfidf[doc]\n",
        "\n",
        "# Print the first five weights\n",
        "print(tfidf_weights[:5])\n",
        "\n",
        "# Sort the weights from highest to lowest: sorted_tfidf_weights\n",
        "sorted_tfidf_weights = sorted(tfidf_weights, key=lambda w: w[1], reverse=True)\n",
        "\n",
        "# Print the top 5 weighted words\n",
        "for term_id, weight in sorted_tfidf_weights[:5]:\n",
        "    print(dictionary.get(term_id), weight)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ssu9feNNAb8o",
        "colab_type": "text"
      },
      "source": [
        "## NER (Name Entity Recognization) with NLTK "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AXAzQbUQk6Tn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Tokenize the article into sentences: sentences\n",
        "sentences = nltk.sent_tokenize(article)\n",
        "\n",
        "# Tokenize each sentence into words: token_sentences\n",
        "token_sentences = [word_tokenize(sent) for sent in sentences]\n",
        "\n",
        "# Tag each tokenized sentence into parts of speech: pos_sentences\n",
        "pos_sentences = [nltk.pos_tag(sent) for sent in token_sentences] \n",
        "\n",
        "# Create the named entity chunks: chunked_sentences\n",
        "chunked_sentences = nltk.ne_chunk_sents(pos_sentences,binary=True)\n",
        "\n",
        "# Test for stems of the tree with 'NE' tags\n",
        "for sent in chunked_sentences:\n",
        "    for chunk in sent:\n",
        "        if hasattr(chunk, \"label\") and chunk.label() == \"NE\":\n",
        "            print(chunk)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rVHX7kAYk6SD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create the defaultdict: ner_categories\n",
        "ner_categories = defaultdict(int)\n",
        "\n",
        "# Create the nested for loop\n",
        "for sent in chunked_sentences:\n",
        "    for chunk in sent:\n",
        "        if hasattr(chunk, 'label'):\n",
        "            ner_categories[chunk.label()] += 1\n",
        "            \n",
        "# Create a list from the dictionary keys for the chart labels: labels\n",
        "labels = list(ner_categories.keys())\n",
        "\n",
        "# Create a list of the values: values\n",
        "values = [ner_categories.get(l) for l in labels]\n",
        "\n",
        "# Create the pie chart\n",
        "plt.pie(values, labels=labels, autopct='%1.1f%%', startangle=140)\n",
        "\n",
        "# Display the chart\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sIbrdwLpCpBG",
        "colab_type": "text"
      },
      "source": [
        "## Comparing NLTK with spaCy NER(amed-entity recognition)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bvqWm74xAlWo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import spacy\n",
        "import spacy\n",
        "\n",
        "# Instantiate the English model: nlp\n",
        "nlp = spacy.load('en',tagger=False, parser=False, matcher=False)\n",
        "\n",
        "# Create a new document: doc\n",
        "doc = nlp(article)\n",
        "\n",
        "# Print all of the found entities and their labels\n",
        "for ent in doc.ents:\n",
        "    print(ent.label_,ent.text)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYpC6_OAD6DA",
        "colab_type": "text"
      },
      "source": [
        "## French NER with polyglot I\n",
        "\n",
        "* As it supports 131 languages and can detect any languate without we telling it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5s26Q_xOD5aW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create a new text object using Polyglot's Text class: txt\n",
        "txt = Text(article)\n",
        "\n",
        "# Print each of the entities found\n",
        "for ent in txt.entities:\n",
        "    print(ent)\n",
        "    \n",
        "# Print the type of ent\n",
        "print(type(ent))\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kym_meIXAlZ1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create the list of tuples: entities\n",
        "entities = [(ent.tag, ' '.join(ent)) for ent in txt.entities]\n",
        "\n",
        "# Print entities\n",
        "print(entities)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CD-cQxXxH-yM",
        "colab_type": "text"
      },
      "source": [
        "## CountVectorizer for text classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rUra_IZnAlfs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import the necessary modules\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Print the head of df\n",
        "print(df.head())\n",
        "\n",
        "# Create a series to store the labels: y\n",
        "y = df.label\n",
        "\n",
        "# Create training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['text'],y,test_size=0.33,random_state=53)\n",
        "\n",
        "# Initialize a CountVectorizer object: count_vectorizer\n",
        "count_vectorizer = CountVectorizer(stop_words='english')\n",
        "\n",
        "# Transform the training data using only the 'text' column values: count_train \n",
        "count_train = count_vectorizer.fit_transform(X_train)\n",
        "\n",
        "# Transform the test data using only the 'text' column values: count_test \n",
        "count_test = count_vectorizer.transform(X_test)\n",
        "\n",
        "# Print the first 10 features of the count_vectorizer\n",
        "print(count_vectorizer.get_feature_names()[:10])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nV3XuRrUAlky",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Initialize a TfidfVectorizer object: tfidf_vectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer(stop_words='english',max_df=0.7)\n",
        "\n",
        "# Transform the training data: tfidf_train \n",
        "tfidf_train = tfidf_vectorizer.fit_transform(X_train)\n",
        "\n",
        "# Transform the test data: tfidf_test \n",
        "tfidf_test = tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "# Print the first 10 features\n",
        "print(tfidf_vectorizer.get_feature_names()[:10])\n",
        "\n",
        "# Print the first 5 vectors of the tfidf training data\n",
        "print(tfidf_train.A[:5])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0NYQYUIwAltQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create the CountVectorizer DataFrame: count_df\n",
        "count_df = pd.DataFrame(count_train.A, columns=count_vectorizer.get_feature_names())\n",
        "\n",
        "# Create the TfidfVectorizer DataFrame: tfidf_df\n",
        "tfidf_df = pd.DataFrame(tfidf_train.A, columns=tfidf_vectorizer.get_feature_names())\n",
        "\n",
        "# Print the head of count_df\n",
        "print(count_df.head())\n",
        "\n",
        "# Print the head of tfidf_df\n",
        "print(tfidf_df.head())\n",
        "\n",
        "# Calculate the difference in columns: difference\n",
        "difference = set(count_df.columns) - set(tfidf_df.columns)\n",
        "print(difference)\n",
        "\n",
        "# Check whether the DataFrames are equal\n",
        "print(count_df.equals(tfidf_df))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M0pDnfxyOYkJ",
        "colab_type": "text"
      },
      "source": [
        "## Training and testing the \"fake news\" model with CountVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e7pZWU03Al1T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import the necessary modules\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn import metrics\n",
        "\n",
        "# Instantiate a Multinomial Naive Bayes classifier: nb_classifier\n",
        "nb_classifier = MultinomialNB()\n",
        "\n",
        "# Fit the classifier to the training data\n",
        "nb_classifier.fit(count_train, y_train)\n",
        "\n",
        "# Create the predicted tags: pred\n",
        "pred = nb_classifier.predict(count_test)\n",
        "\n",
        "# Calculate the accuracy score: score\n",
        "score = metrics.accuracy_score(y_test, pred)\n",
        "print(score)\n",
        "\n",
        "# Calculate the confusion matrix: cm\n",
        "cm = metrics.confusion_matrix(y_test, pred, labels=['FAKE', 'REAL'])\n",
        "print(cm)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4e_OxOfKOd66",
        "colab_type": "text"
      },
      "source": [
        "## Training and testing the \"fake news\" model with TfidfVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O16nMZOSAl4v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create a Multinomial Naive Bayes classifier: nb_classifier\n",
        "nb_classifier = MultinomialNB()\n",
        "\n",
        "# Fit the classifier to the training data\n",
        "nb_classifier.fit(tfidf_train, y_train)\n",
        "\n",
        "# Create the predicted tags: pred\n",
        "pred = nb_classifier.predict(tfidf_test)\n",
        "\n",
        "# Calculate the accuracy score: score\n",
        "score = metrics.accuracy_score(y_test, pred)\n",
        "print(score)\n",
        "\n",
        "# Calculate the confusion matrix: cm\n",
        "cm = metrics.confusion_matrix(y_test, pred, labels=['FAKE', 'REAL'])\n",
        "print(cm)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yKYw6f40Alyp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create the list of alphas: alphas\n",
        "alphas = np.arange(0, 1, .1)\n",
        "\n",
        "# Define train_and_predict()\n",
        "def train_and_predict(alpha):\n",
        "    # Instantiate the classifier: nb_classifier\n",
        "    nb_classifier = MultinomialNB(alpha=alpha)\n",
        "    # Fit to the training data\n",
        "    nb_classifier.fit(tfidf_train, y_train)\n",
        "    # Predict the labels: pred\n",
        "    pred = nb_classifier.predict(tfidf_test)\n",
        "    # Compute accuracy: score\n",
        "    score = metrics.accuracy_score(y_test, pred)\n",
        "    return score\n",
        "\n",
        "# Iterate over the alphas and print the corresponding score\n",
        "for alpha in alphas:\n",
        "    print('Alpha: ', alpha)\n",
        "    print('Score: ', train_and_predict(alpha))\n",
        "    print()\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-qVUqaeAlxN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get the class labels: class_labels\n",
        "class_labels = nb_classifier.classes_\n",
        "\n",
        "# Extract the features: feature_names\n",
        "feature_names = tfidf_vectorizer.get_feature_names()\n",
        "\n",
        "# Zip the feature names together with the coefficient array and sort by weights: feat_with_weights\n",
        "feat_with_weights = sorted(zip(nb_classifier.coef_[0], feature_names))\n",
        "\n",
        "# Print the first class label and the top 20 feat_with_weights entries\n",
        "print(class_labels[0], feat_with_weights[:20])\n",
        "\n",
        "# Print the second class label and the bottom 20 feat_with_weights entries\n",
        "print(class_labels[1], feat_with_weights[-20:])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GRb_7Wjx0fIo",
        "colab_type": "text"
      },
      "source": [
        "# Feature Engineering for NLP in Python\n",
        "\n",
        "## Hashtags and mentions in Russian tweets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W6pu0MRVAldM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function that returns number of hashtags in a string\n",
        "def count_hashtags(string):\n",
        "\t# Split the string into words\n",
        "    words = string.split()\n",
        "    \n",
        "    # Create a list of words that are hashtags\n",
        "    hashtags = [word for word in words if word.startswith('#')]\n",
        "    \n",
        "    # Return number of hashtags\n",
        "    return(len(hashtags))\n",
        "\n",
        "# Create a feature hashtag_count and display distribution\n",
        "tweets['hashtag_count'] = tweets['content'].apply(count_hashtags)\n",
        "tweets['hashtag_count'].hist()\n",
        "plt.title('Hashtag count distribution')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vXfZen-w0zav",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function that returns number of mentions in a string\n",
        "def count_mentions(string):\n",
        "\t# Split the string into words\n",
        "    words = string.split()\n",
        "    \n",
        "    # Create a list of words that are mentions\n",
        "    mentions = [word for word in words if word.startswith('@')]\n",
        "    \n",
        "    # Return number of mentions\n",
        "    return(len(mentions))\n",
        "\n",
        "# Create a feature mention_count and display distribution\n",
        "tweets['mention_count'] = tweets['content'].apply(count_mentions)\n",
        "tweets['mention_count'].hist()\n",
        "plt.title('Mention count distribution')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dUKQrW-V5GVQ",
        "colab_type": "text"
      },
      "source": [
        "## Tokenizing the Gettysburg Address\n",
        "\n",
        "## Lemmatizing the Gettysburg address"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Jh1Jrvn0zd9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import spacy\n",
        "\n",
        "# Load the en_core_web_sm model\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Create a Doc object\n",
        "doc = nlp(gettysburg)\n",
        "\n",
        "# Generate the tokens\n",
        "tokens = [token.text for token in doc]\n",
        "print(tokens)\n",
        "\n",
        "#----------------------------------------------------\n",
        "import spacy\n",
        "\n",
        "# Load the en_core_web_sm model\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Create a Doc object\n",
        "doc = nlp(gettysburg)\n",
        "\n",
        "# Generate lemmas\n",
        "lemmas = [token.lemma_ for token in doc]\n",
        "\n",
        "# Convert lemmas into a string\n",
        "print(' '.join(lemmas))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nIV4hHQ26scV",
        "colab_type": "text"
      },
      "source": [
        "## Cleaning a blog post"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LW8rLytk0zjy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load model and create Doc object\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "doc = nlp(blog)\n",
        "\n",
        "# Generate lemmatized tokens\n",
        "lemmas = [token.lemma_ for token in doc]\n",
        "\n",
        "# Remove stopwords and non-alphabetic tokens\n",
        "a_lemmas = [lemma for lemma in lemmas \n",
        "            if lemma.isalpha() and lemma not in stopwords]\n",
        "\n",
        "# Print string after text cleaning\n",
        "print(' '.join(a_lemmas))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ycsvVLn20zpj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function to preprocess text\n",
        "def preprocess(text):\n",
        "  \t# Create Doc object\n",
        "    doc = nlp(text, disable=['ner', 'parser'])\n",
        "    # Generate lemmas\n",
        "    lemmas = [token.lemma_ for token in doc]\n",
        "    # Remove stopwords and non-alphabetic characters\n",
        "    a_lemmas = [lemma for lemma in lemmas \n",
        "            if lemma.isalpha() and lemma not in stopwords]\n",
        "    \n",
        "    return ' '.join(a_lemmas)\n",
        "  \n",
        "# Apply preprocess to ted['transcript']\n",
        "ted['transcript'] = ted['transcript'].apply(preprocess)\n",
        "print(ted['transcript'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gs_7Cw1rJn23",
        "colab_type": "text"
      },
      "source": [
        "## POS tagging in Lord of the Flies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t4nTb7r20zvW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load the en_core_web_sm model\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Create a Doc object\n",
        "doc = nlp(lotf)\n",
        "\n",
        "# Generate tokens and pos tags\n",
        "pos = [(token.text, token.pos_) for token in doc]\n",
        "print(pos)\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Returns number of proper nouns\n",
        "def proper_nouns(text, model=nlp):\n",
        "  \t# Create doc object\n",
        "    doc = model(text)\n",
        "    # Generate list of POS tags\n",
        "    pos = [token.pos_ for token in doc]\n",
        "    \n",
        "    # Return number of proper nouns\n",
        "    return pos.count('PROPN')\n",
        "\n",
        "print(proper_nouns(\"Abdul, Bill and Cathy went to the market to buy apples.\", nlp))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_iYT6uoe0z18",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Returns number of other nouns\n",
        "def nouns(text, model=nlp):\n",
        "  \t# Create doc object\n",
        "    doc = model(text)\n",
        "    # Generate list of POS tags\n",
        "    pos = [token.pos_ for token in doc]\n",
        "    \n",
        "    # Return number of other nouns\n",
        "    return pos.count('NOUN')\n",
        "\n",
        "print(nouns(\"Abdul, Bill and Cathy went to the market to buy apples.\", nlp))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lC1W0Y7N0z5Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "headlines['num_propn'] = headlines['title'].apply(proper_nouns)\n",
        "headlines['num_noun'] = headlines['title'].apply(nouns)\n",
        "\n",
        "# Compute mean of proper nouns\n",
        "real_propn = headlines[headlines['label'] == 'REAL']['num_propn'].mean()\n",
        "fake_propn = headlines[headlines['label'] == 'FAKE']['num_propn'].mean()\n",
        "\n",
        "# Compute mean of other nouns\n",
        "real_noun = headlines[headlines['label'] == 'REAL']['num_noun'].mean()\n",
        "fake_noun = headlines[headlines['label'] == 'FAKE']['num_noun'].mean()\n",
        "\n",
        "# Print results\n",
        "print(\"Mean no. of proper nouns in real and fake headlines are %.2f and %.2f respectively\"%(real_propn, fake_propn))\n",
        "print(\"Mean no. of other nouns in real and fake headlines are %.2f and %.2f respectively\"%(real_noun, fake_noun))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2OKSIB0TPxB",
        "colab_type": "text"
      },
      "source": [
        "## Named entities(NER) in a sentence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_bUkIjbx0z8Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load the required model\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Create a Doc instance \n",
        "text = 'Sundar Pichai is the CEO of Google. Its headquarters is in Mountain View.'\n",
        "doc = nlp(text)\n",
        "\n",
        "# Print all named entities and their labels\n",
        "for ent in doc.ents:\n",
        "    print(ent.text, ent.label_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u9in5y8g0zzb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def find_persons(text):\n",
        "  # Create Doc object\n",
        "  doc = nlp(text)\n",
        "  \n",
        "  # Identify the persons\n",
        "  persons = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\n",
        "  \n",
        "  # Return persons\n",
        "  return persons\n",
        "\n",
        "print(find_persons(tc))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ye2wZ0Pp0ztJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import CountVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Create CountVectorizer object\n",
        "vectorizer = CountVectorizer(stop_words ='english')\n",
        "\n",
        "# Generate matrix of word vectors\n",
        "bow_matrix = vectorizer.fit_transform(corpus)\n",
        "\n",
        "# Print the shape of bow_matrix\n",
        "print(bow_matrix.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uuirR6Vc0zni",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import CountVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Create CountVectorizer object\n",
        "vectorizer = CountVectorizer\n",
        "\n",
        "# Generate matrix of word vectors\n",
        "bow_lem_matrix = vectorizer.fit_transform(lem_corpus)\n",
        "\n",
        "# Print the shape of bow_lem_matrix\n",
        "print(bow_lem_matrix.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BkOFhdYRg43N",
        "colab_type": "text"
      },
      "source": [
        "## Mapping feature indices with feature names"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mtHfvEY70zgr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create CountVectorizer object\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "# Generate matrix of word vectors\n",
        "bow_matrix = vectorizer.fit_transform(corpus)\n",
        "\n",
        "# Convert bow_matrix into a DataFrame\n",
        "bow_df = pd.DataFrame(bow_matrix.toarray())\n",
        "\n",
        "# Map the column names to vocabulary \n",
        "bow_df.columns = vectorizer.get_feature_names()\n",
        "\n",
        "# Print bow_df\n",
        "print(bow_df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3l18TQkCivug",
        "colab_type": "text"
      },
      "source": [
        "## BoW vectors for movie reviews"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tXwb7ldbg8IH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import CountVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Create a CountVectorizer object\n",
        "vectorizer = CountVectorizer(lowercase=True, stop_words='english')\n",
        "\n",
        "# Fit and transform X_train\n",
        "X_train_bow = vectorizer.fit_transform(X_train)\n",
        "\n",
        "# Transform X_test\n",
        "X_test_bow = vectorizer.transform(X_test)\n",
        "\n",
        "# Print shape of X_train_bow and X_test_bow\n",
        "print(X_train_bow.shape)\n",
        "print(X_test_bow.shape)\n",
        "\n",
        "# Create a MultinomialNB object\n",
        "clf = MultinomialNB()\n",
        "\n",
        "# Fit the classifier\n",
        "clf.fit(X_train_bow, y_train)\n",
        "\n",
        "# Measure the accuracy\n",
        "accuracy = clf.score(X_test_bow, y_test)\n",
        "print(\"The accuracy of the classifier on the test set is %.3f\" % accuracy)\n",
        "\n",
        "# Predict the sentiment of a negative review\n",
        "review = \"The movie was terrible. The music was underwhelming and the acting mediocre.\"\n",
        "prediction = clf.predict(vectorizer.transform([review]))[0]\n",
        "print(\"The sentiment predicted by the classifier is %i\" % (prediction))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fKIR22y5oRG1",
        "colab_type": "text"
      },
      "source": [
        "## n-gram models for movie tag lines"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pWZzzeWjg8RH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Generate n-grams upto n=1\n",
        "vectorizer_ng1 = CountVectorizer(ngram_range=(1,1))\n",
        "ng1 = vectorizer_ng1.fit_transform(corpus)\n",
        "\n",
        "# Generate n-grams upto n=2\n",
        "vectorizer_ng2 = CountVectorizer(ngram_range=(1,2))\n",
        "ng2 = vectorizer_ng2.fit_transform(corpus)\n",
        "\n",
        "# Generate n-grams upto n=3\n",
        "vectorizer_ng3 = CountVectorizer(ngram_range=(1, 3))\n",
        "ng3 = vectorizer_ng3.fit_transform(corpus)\n",
        "\n",
        "# Print the number of features for each model\n",
        "print(\"ng1, ng2 and ng3 have %i, %i and %i features respectively\" % (ng1.shape[1], ng2.shape[1], ng3.shape[1]))\n",
        "\n",
        "# Define an instance of MultinomialNB \n",
        "clf_ng = MultinomialNB()\n",
        "\n",
        "# Fit the classifier \n",
        "clf_ng.fit(X_train_ng, y_train)\n",
        "\n",
        "# Measure the accuracy \n",
        "accuracy = clf_ng.score(X_test_ng,y_test)\n",
        "print(\"The accuracy of the classifier on the test set is %.3f\" % accuracy)\n",
        "\n",
        "# Predict the sentiment of a negative review\n",
        "review = \"The movie was not good. The plot had several holes and the acting lacked panache.\"\n",
        "prediction = clf_ng.predict(ng_vectorizer.transform([review]))[0]\n",
        "print(\"The sentiment predicted by the classifier is %i\" % (prediction))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BaK2PX2QpYi_",
        "colab_type": "text"
      },
      "source": [
        "## Comparing performance of n-gram models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "55SOLGxHg8Um",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "start_time = time.time()\n",
        "# Splitting the data into training and test sets\n",
        "train_X, test_X, train_y, test_y = train_test_split(df['review'], df['sentiment'], test_size=0.5, random_state=42, stratify=df['sentiment'])\n",
        "\n",
        "# Generating ngrams\n",
        "vectorizer = ___\n",
        "train_X = vectorizer.fit_transform(train_X)\n",
        "test_X = vectorizer.transform(test_X)\n",
        "\n",
        "# Fit classifier\n",
        "clf = MultinomialNB()\n",
        "clf.fit(train_X, train_y)\n",
        "\n",
        "# Print accuracy, time and number of dimensions\n",
        "print(\"The program took %.3f seconds to complete. The accuracy on the test set is %.2f. The ngram representation had %i features.\"\\\n",
        "      % (time.time() - start_time, clf.score(test_X, test_y), train_X.shape[1]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OA4KiQ6Mg8ZW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "start_time = time.time()\n",
        "# Splitting the data into training and test sets\n",
        "train_X, test_X, train_y, test_y = train_test_split(df['review'], df['sentiment'], test_size=0.5, random_state=42, stratify=df['sentiment'])\n",
        "\n",
        "# Generating ngrams\n",
        "vectorizer = CountVectorizer(ngram_range=(1,3))\n",
        "train_X = vectorizer.fit_transform(train_X)\n",
        "test_X = vectorizer.transform(test_X)\n",
        "\n",
        "# Fit classifier\n",
        "clf = MultinomialNB()\n",
        "clf.fit(train_X, train_y)\n",
        "\n",
        "# Print accuracy, time and number of dimensions\n",
        "print(\"The program took %.3f seconds to complete. The accuracy on the test set is %.2f. The ngram representation had %i features.\"\\\n",
        "      % (time.time() - start_time, clf.score(test_X, test_y), train_X.shape[1]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06QMGteMtXbN",
        "colab_type": "text"
      },
      "source": [
        "## tf-idf vectors for TED talks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ej_Hg0oUg8fP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Create TfidfVectorizer object\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Generate matrix of word vectors\n",
        "tfidf_matrix = vectorizer.fit_transform(ted)\n",
        "\n",
        "# Print the shape of tfidf_matrix\n",
        "print(tfidf_matrix.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GlYXFEU3xA64",
        "colab_type": "text"
      },
      "source": [
        "## Cosine similarity matrix of a corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bEzEGb4ng8of",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Initialize an instance of tf-idf Vectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Generate the tf-idf vectors for the corpus\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(corpus)\n",
        "\n",
        "# Compute and print the cosine similarity matrix\n",
        "cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
        "print(cosine_sim)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3rPzsLgy1FC",
        "colab_type": "text"
      },
      "source": [
        "# Building a plot line based recommender\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bPIc5UOEg8mF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Record start time\n",
        "start = time.time()\n",
        "\n",
        "# Compute cosine similarity matrix\n",
        "cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
        "\n",
        "# Print cosine similarity matrix\n",
        "print(cosine_sim)\n",
        "\n",
        "# Print time taken\n",
        "print(\"Time taken: %s seconds\" %(time.time() - start))  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m9Evn7sEg8jI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Record start time\n",
        "start = time.time()\n",
        "\n",
        "# Compute cosine similarity matrix\n",
        "cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)\n",
        "\n",
        "# Print cosine similarity matrix\n",
        "print(cosine_sim)\n",
        "\n",
        "# Print time taken\n",
        "print(\"Time taken: %s seconds\" %(time.time() - start))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bWGrYUl6g8c0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Initialize the TfidfVectorizer \n",
        "tfidf = TfidfVectorizer(stop_words='english')\n",
        "\n",
        "# Construct the TF-IDF matrix\n",
        "tfidf_matrix = tfidf.fit_transform(movie_plots)\n",
        "\n",
        "# Generate the cosine similarity matrix\n",
        "cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)\n",
        " \n",
        "# Generate recommendations \n",
        "print(get_recommendations('The Dark Knight Rises', cosine_sim, indices))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5d7q2RaA0-fa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Generate mapping between titles and index\n",
        "indices = pd.Series(metadata.index, index=metadata['title']).drop_duplicates()\n",
        "\n",
        "def get_recommendations(title, cosine_sim, indices):\n",
        "    # Get index of movie that matches title\n",
        "    idx = indices[title]\n",
        "    # Sort the movies based on the similarity scores\n",
        "    sim_scores = list(enumerate(cosine_sim[idx]))\n",
        "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
        "    # Get the scores for 10 most similar movies\n",
        "    sim_scores = sim_scores[1:11]\n",
        "    # Get the movie indices\n",
        "    movie_indices = [i[0] for i in sim_scores]\n",
        "    # Return the top 10 most similar movies\n",
        "    return metadata['title'].iloc[movie_indices]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7bbXKmkj0-o4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Initialize the TfidfVectorizer \n",
        "tfidf = TfidfVectorizer(stop_words='english')\n",
        "\n",
        "# Construct the TF-IDF matrix\n",
        "tfidf_matrix = tfidf.fit_transform(transcripts)\n",
        "\n",
        "# Generate the cosine similarity matrix\n",
        "cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)\n",
        " \n",
        "# Generate recommendations\n",
        "print(get_recommendations('5 ways to kill your dreams', cosine_sim, indices))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "he_SQxkL4Pbk",
        "colab_type": "text"
      },
      "source": [
        "## Generating word vectors using Spacy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GSjmKnON0-zr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create the doc object\n",
        "doc = nlp(sent)\n",
        "\n",
        "# Compute pairwise similarity scores\n",
        "for token1 in doc:\n",
        "  for token2 in doc:\n",
        "    print(token1.text, token2.text, token1.similarity(token2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rYzvBRpd4kUt",
        "colab_type": "text"
      },
      "source": [
        "## Computing similarity of Pink Floyd songs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c0no3Ns30-xR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create Doc objects\n",
        "mother_doc = nlp(mother)\n",
        "hopes_doc = nlp(hopes)\n",
        "hey_doc = nlp(hey)\n",
        "\n",
        "# Print similarity between mother and hopes\n",
        "print(mother_doc.similarity(hopes_doc))\n",
        "\n",
        "# Print similarity between mother and hey\n",
        "print(mother_doc.similarity(hey_doc))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8k9gHBKRg8OQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}