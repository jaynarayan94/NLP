{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentiment Analysis.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jaynarayan94/NLP/blob/master/Sentiment_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lH18NwyDJJMG",
        "colab_type": "text"
      },
      "source": [
        "## Detecting the sentiment of Tale of Two Cities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IfnJPgQjJDfX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import the required packages\n",
        "from textblob import TextBlob\n",
        "\n",
        "# Create a textblob object \n",
        "blob_two_cities = TextBlob(two_cities)\n",
        "\n",
        "# Print out the sentiment   \n",
        "print(blob_two_cities.sentiment)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ODUUuu-YJUaM",
        "colab_type": "text"
      },
      "source": [
        "## Comparing the sentiment of two strings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vFV-QNHnJLKr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import the required packages\n",
        "from textblob import TextBlob\n",
        "\n",
        "# Create a textblob object \n",
        "blob_annak = TextBlob(annak)\n",
        "blob_catcher = TextBlob(catcher)\n",
        "\n",
        "# Print out the sentiment   \n",
        "print('Sentiment of annak: ', blob_annak.sentiment)\n",
        "print('Sentiment of catcher: ', blob_catcher.sentiment)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1R75SJZwJLOF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# What is the sentiment of a movie review?\n",
        "# Import the required packages\n",
        "from textblob import TextBlob\n",
        "\n",
        "# Create a textblob object  \n",
        "blob_titanic = TextBlob(titanic)\n",
        "\n",
        "# Print out its sentiment  \n",
        "print(blob_titanic.sentiment)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iOoulppHLip1",
        "colab_type": "text"
      },
      "source": [
        "## Word cloud"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VAhWkSA_JLQ7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Generate the word cloud from the east_of_eden string\n",
        "cloud_east_of_eden = WordCloud(background_color=\"white\").generate(east_of_eden)\n",
        "\n",
        "# Create a figure of the generated cloud\n",
        "plt.imshow(cloud_east_of_eden, interpolation='bilinear')  \n",
        "plt.axis('off')\n",
        "# Display the figure\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yoPuyJbUMgPM",
        "colab_type": "text"
      },
      "source": [
        "## Word Cloud on movie reviews"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GVhlnfLjJLTt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import the word cloud function  \n",
        "from wordcloud import WordCloud\n",
        "\n",
        "# Create and generate a word cloud image \n",
        "my_cloud = WordCloud(background_color='white', stopwords=my_stopwords).generate(descriptions)\n",
        "\n",
        "# Display the generated wordcloud image\n",
        "plt.imshow(my_cloud, interpolation='bilinear') \n",
        "plt.axis(\"off\")\n",
        "\n",
        "# Don't forget to show the final image\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X1zcaB6VzV_g",
        "colab_type": "text"
      },
      "source": [
        "## BOW\n",
        "A bag-of-words is an approach to transform text to numeric form."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XglN03lcJLWr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import the required function\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "annak = ['Happy families are all alike;', 'every unhappy family is unhappy in its own way']\n",
        "\n",
        "# Build the vectorizer and fit it\n",
        "anna_vect = CountVectorizer()\n",
        "anna_vect.fit(annak)\n",
        "\n",
        "# Create the bow representation\n",
        "anna_bow = anna_vect.transform(annak)\n",
        "\n",
        "# Print the bag-of-words result \n",
        "print(anna_bow.toarray())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FqjlsKIPJLZH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer \n",
        "\n",
        "# Build the vectorizer, specify max features \n",
        "vect = CountVectorizer(max_features=100)\n",
        "# Fit the vectorizer\n",
        "vect.fit(reviews.review)\n",
        "\n",
        "# Transform the review column\n",
        "X_review = vect.transform(reviews.review)\n",
        "\n",
        "# Create the bow representation\n",
        "X_df=pd.DataFrame(X_review.toarray(), columns=vect.get_feature_names())\n",
        "print(X_df.head())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iZFnvP-J3Ni2",
        "colab_type": "text"
      },
      "source": [
        "## Specify token sequence length with BOW"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YIeRK1vaJLcO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer \n",
        "\n",
        "# Build the vectorizer, specify token sequence and fit\n",
        "vect = CountVectorizer(ngram_range=(1,2))\n",
        "vect.fit(reviews.review)\n",
        "\n",
        "# Transform the review column\n",
        "X_review = vect.transform(reviews.review)\n",
        "\n",
        "# Create the bow representation\n",
        "X_df = pd.DataFrame(X_review.toarray(), columns=vect.get_feature_names())\n",
        "print(X_df.head())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2v3rEsDsJLe_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer \n",
        "\n",
        "# Build and fit the vectorizer\n",
        "vect = CountVectorizer(max_features=200)\n",
        "vect.fit(movies.review)\n",
        "\n",
        "# Transform the review column\n",
        "X_review = vect.transform(movies.review)\n",
        "# Create the bow representation\n",
        "X_df = pd.DataFrame(X_review.toarray(), columns=vect.get_feature_names())\n",
        "print(X_df.head())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fWTKG8x3tXz",
        "colab_type": "text"
      },
      "source": [
        "## Using the movies dataset, limit the size of the vocabulary to 100"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AtpMyK7-3tmy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer \n",
        "\n",
        "# Build the vectorizer, specify size of vocabulary and fit\n",
        "vect = CountVectorizer(max_features=100)\n",
        "vect.fit(movies.review)\n",
        "\n",
        "# Transform the review column\n",
        "X_review = vect.transform(movies.review)\n",
        "# Create the bow representation\n",
        "X_df = pd.DataFrame(X_review.toarray(), columns=vect.get_feature_names())\n",
        "print(X_df.head())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lsgxw1hr3rZb",
        "colab_type": "text"
      },
      "source": [
        "## Using the movies dataset, limit the size of the vocabulary to include terms which occur in no more than 200 documents."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ViJ6PgxJLiI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer \n",
        "\n",
        "# Build and fit the vectorizer\n",
        "vect = CountVectorizer(max_df=200)\n",
        "vect.fit(movies.review)\n",
        "\n",
        "# Transform the review column\n",
        "X_review = vect.transform(movies.review)\n",
        "# Create the bow representation\n",
        "X_df = pd.DataFrame(X_review.toarray(), columns=vect.get_feature_names())\n",
        "print(X_df.head())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FAI7fxo93zbZ",
        "colab_type": "text"
      },
      "source": [
        "## Using the movies dataset, limit the size of the vocabulary to ignore terms which occur in less than 50 documents."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rPMPsZGIJLk_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer \n",
        "\n",
        "# Build and fit the vectorizer\n",
        "vect = CountVectorizer(min_df=50)\n",
        "vect.fit(movies.review)\n",
        "\n",
        "# Transform the review column\n",
        "X_review = vect.transform(movies.review)\n",
        "# Create the bow representation\n",
        "X_df = pd.DataFrame(X_review.toarray(), columns=vect.get_feature_names())\n",
        "print(X_df.head())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eKsj3xlT4YXm",
        "colab_type": "text"
      },
      "source": [
        "Ques : Build the vectorizer and make sure to specify the following parameters: the size of the vocabulary should be limited to 1000, include only bigrams, and ignore terms that appear in more than 500 documents."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pJlEd4VuJLn2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Import the vectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Build the vectorizer, specify max features and fit\n",
        "vect = CountVectorizer(max_features=1000, ngram_range=(2, 2), max_df=500)\n",
        "vect.fit(reviews.review)\n",
        "\n",
        "# Transform the review\n",
        "X_review = vect.transform(reviews.review)\n",
        "\n",
        "# Create a DataFrame from the bow representation\n",
        "X_df = pd.DataFrame(X_review.toarray(), columns=vect.get_feature_names())\n",
        "print(X_df.head())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aALzUVe6JLq4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import the word tokenizing function\n",
        "from nltk import word_tokenize\n",
        "\n",
        "# Tokenize each item in the avengers \n",
        "tokens_avengers = [word_tokenize(item) for item in avengers]\n",
        "\n",
        "print(tokens_avengers)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a0pUgInA4bXU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create an empty list to store the length of the reviews\n",
        "len_tokens = []\n",
        "\n",
        "# Iterate over the word_tokens list and determine the length of each item\n",
        "for i in range(len(word_tokens)):\n",
        "     len_tokens.append(len(word_tokens[i]))\n",
        "\n",
        "# Create a new feature for the lengh of each review\n",
        "reviews['n_words'] = len_tokens "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qP1mLN3_8SyK",
        "colab_type": "text"
      },
      "source": [
        "## Identify the language of a string"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2qKygNnP4bag",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import the language detection function and package\n",
        "from langdetect import detect_langs\n",
        "\n",
        "# Detect the language of the foreign string\n",
        "print(detect_langs(foreign))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H_EplFm14bdk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from langdetect import detect_langs\n",
        "\n",
        "languages = []\n",
        "\n",
        "# Loop over the sentences in the list and detect their language\n",
        "for sentence in range(len(sentences)):\n",
        "    languages.append(detect_langs(sentences[sentence]))\n",
        "    \n",
        "print('The detected languages are: ', languages)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4nmtdkSc4bga",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from langdetect import detect_langs\n",
        "languages = [] \n",
        "\n",
        "# Loop over the rows of the dataset and append  \n",
        "for row in range(len(non_english_reviews)):\n",
        "    languages.append(detect_langs(non_english_reviews.iloc[row, 1]))\n",
        "\n",
        "# Clean the list by splitting     \n",
        "languages = [str(lang).split(':')[0][1:] for lang in languages]\n",
        "\n",
        "# Assign the list to a new feature \n",
        "non_english_reviews['language'] = languages\n",
        "\n",
        "print(non_english_reviews.head())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pyb1kD3K91s6",
        "colab_type": "text"
      },
      "source": [
        "## Word cloud of tweets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bvo6nsSO4bot",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import the word cloud function \n",
        "from wordcloud import WordCloud \n",
        "\n",
        "# Create and generate a word cloud image\n",
        "my_cloud = WordCloud(background_color='white').generate(text_tweet)\n",
        "\n",
        "# Display the generated wordcloud image\n",
        "plt.imshow(my_cloud, interpolation='bilinear') \n",
        "plt.axis(\"off\")\n",
        "\n",
        "# Don't forget to show the final image\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D3ISGLfN4bsR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import the word cloud function and stop words list\n",
        "from wordcloud import WordCloud, STOPWORDS \n",
        "\n",
        "# Define and update the list of stopwords\n",
        "my_stop_words = STOPWORDS.update(['airline', 'airplane'])\n",
        "\n",
        "# Create and generate a word cloud image\n",
        "my_cloud = WordCloud(stopwords=my_stop_words).generate(text_tweet)\n",
        "\n",
        "# Display the generated wordcloud image\n",
        "plt.imshow(my_cloud, interpolation='bilinear') \n",
        "plt.axis(\"off\")\n",
        "# Don't forget to show the final image\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJPDE9VJ4bxm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import the stop words\n",
        "from sklearn.feature_extraction.text import CountVectorizer, ENGLISH_STOP_WORDS\n",
        "\n",
        "# Define the stop words\n",
        "my_stop_words = ENGLISH_STOP_WORDS.union(['airline', 'airlines', '@'])\n",
        "\n",
        "# Build and fit the vectorizer\n",
        "vect = CountVectorizer(stop_words=my_stop_words)\n",
        "vect.fit(tweets.text)\n",
        "\n",
        "# Create the bow representation\n",
        "X_review = vect.transform(tweets.text)\n",
        "# Create the data frame\n",
        "X_df = pd.DataFrame(X_review.toarray(), columns=vect.get_feature_names())\n",
        "print(X_df.head())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a21TVaUF4b3i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import the vectorizer and default English stop words list\n",
        "from sklearn.feature_extraction.text import CountVectorizer, ENGLISH_STOP_WORDS \n",
        "\n",
        "# Define the stop words\n",
        "my_stop_words = ENGLISH_STOP_WORDS.union(['airline', 'airlines', '@', 'am', 'pm'])\n",
        " \n",
        "# Build and fit the vectorizers\n",
        "vect1 = CountVectorizer(stop_words=my_stop_words)\n",
        "vect2 = CountVectorizer(stop_words=ENGLISH_STOP_WORDS)\n",
        "vect1.fit(tweets.text)\n",
        "vect2.fit(tweets.negative_reason)\n",
        "\n",
        "# Print the last 15 features from the first, and all from second vectorizer\n",
        "print(vect1.get_feature_names()[-15:])\n",
        "print(vect2.get_feature_names())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8nMQLzT7AIbf",
        "colab_type": "text"
      },
      "source": [
        "## Specify the token pattern"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BBhqbPJz4b61",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Build and fit the vectorizer\n",
        "vect = CountVectorizer(token_pattern=r'\\b[^\\d\\W][^\\d\\W]').fit(tweets.text)\n",
        "vect.transform(tweets.text)\n",
        "print('Length of vectorizer: ', len(vect.get_feature_names()))s"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZJxE-qs4b9z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Build the first vectorizer\n",
        "vect1 = CountVectorizer().fit(tweets.text)\n",
        "vect1.transform(tweets.text)\n",
        "\n",
        "# Build the second vectorizer\n",
        "vect2 = CountVectorizer(token_pattern=r'\\b[^\\d\\W][^\\d\\W]').fit(tweets.text)\n",
        "vect2.transform(tweets.text)\n",
        "\n",
        "# Print out the length of each vectorizer\n",
        "print('Length of vectorizer 1: ', len(vect1.get_feature_names()))\n",
        "print('Length of vectorizer 2: ', len(vect2.get_feature_names()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-yCOIh8o4b1Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import the word tokenizing package\n",
        "from nltk import word_tokenize\n",
        "\n",
        "# Tokenize the text column\n",
        "word_tokens = [word_tokenize(review) for review in tweets.text]\n",
        "print('Original tokens: ', word_tokens[0])\n",
        "\n",
        "# Filter out non-letter characters\n",
        "cleaned_tokens = [[word for word in item if word.isalpha()] for item in word_tokens]\n",
        "print('Cleaned tokens: ', cleaned_tokens[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8UPAljPx4bva",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create a list of lists, containing the tokens from list_tweets\n",
        "tokens = [word_tokenize(item) for item in tweets_list]\n",
        "\n",
        "# Remove characters and digits , i.e. retain only letters\n",
        "letters = [[word for word in item if word.isalpha()] for item in tokens]\n",
        "# Remove characters, i.e. retain only letters and digits\n",
        "let_digits = [[word for word in item if word.isalnum()] for item in tokens]\n",
        "# Remove letters and characters, retain only digits\n",
        "digits = [[word for word in item if word.isdigit()] for item in tokens]\n",
        "\n",
        "# Print the last item in each list\n",
        "print('Last item in alphabetic list: ', letters[2])\n",
        "print('Last item in list of alphanumerics: ', let_digits[2])\n",
        "print('Last item in the list of digits: ', digits[2])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I36paMCTNE-0",
        "colab_type": "text"
      },
      "source": [
        "## Stems and lemmas from GoT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jptIsuBN4bkj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import the required packages from nltk\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk import word_tokenize\n",
        "\n",
        "porter = PorterStemmer()\n",
        "WNlemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Tokenize the GoT string\n",
        "tokens = word_tokenize(GoT)\n",
        "\n",
        "import time\n",
        "\n",
        "# Log the start time\n",
        "start_time = time.time()\n",
        "\n",
        "# Build a stemmed list\n",
        "stemmed_tokens = [porter.stem(token) for token in tokens] \n",
        "\n",
        "# Log the end time\n",
        "end_time = time.time()\n",
        "\n",
        "print('Time taken for stemming in seconds: ', end_time - start_time)\n",
        "print('Stemmed tokens: ', stemmed_tokens) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kPQw6Hx9JLtq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "\n",
        "# Log the start time\n",
        "start_time = time.time()\n",
        "\n",
        "# Build a lemmatized list\n",
        "lem_tokens = [WNlemmatizer.lemmatize(token) for token in tokens]\n",
        "\n",
        "# Log the end time\n",
        "end_time = time.time()\n",
        "\n",
        "print('Time taken for lemmatizing in seconds: ', end_time - start_time)\n",
        "print('Lemmatized tokens: ', lem_tokens) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9kgm5ZXeNotQ",
        "colab_type": "text"
      },
      "source": [
        "## Stem Spanish reviews"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2G-GWw3gJLws",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import the language detection package\n",
        "import langdetect\n",
        "\n",
        "# Loop over the rows of the dataset and append  \n",
        "languages = [] \n",
        "for i in range(len(non_english_reviews)):\n",
        "    languages.append(langdetect.detect_langs(non_english_reviews.iloc[i, 1]))\n",
        "\n",
        "# Clean the list by splitting     \n",
        "languages = [str(lang).split(':')[0][1:] for lang in languages]\n",
        "# Assign the list to a new feature \n",
        "non_english_reviews['language'] = languages\n",
        "\n",
        "# Select the Spanish ones\n",
        "non_english_reviews = non_english_reviews[non_english_reviews.language == 'es']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LAOfUH1hJLze",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import the required packages\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from nltk import word_tokenize\n",
        "\n",
        "# Import the Spanish SnowballStemmer\n",
        "SpanishStemmer = SnowballStemmer(\"spanish\")\n",
        "\n",
        "# Create a list of tokens\n",
        "tokens = [word_tokenize(review) for review in non_english_reviews.review] \n",
        "# Stem the list of tokens\n",
        "stemmed_tokens = [[SpanishStemmer.stem(word) for word in token] for token in tokens]\n",
        "\n",
        "# Print the first item of the stemmed tokenss\n",
        "stemmed_tokens[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "daYtBC5MJL5m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import the function to perform stemming\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk import word_tokenize\n",
        "\n",
        "# Call the stemmer\n",
        "porter = PorterStemmer()\n",
        "\n",
        "# Transform the array of tweets to tokens\n",
        "tokens = [word_tokenize(tweet) for tweet in tweets]\n",
        "# Stem the list of tokens\n",
        "stemmed_tokens = [[porter.stem(word) for word in tweet] for tweet in tokens] \n",
        "# Print the first element of the list\n",
        "stemmed_tokens[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMU0HcZRUpMm",
        "colab_type": "text"
      },
      "source": [
        "# TfIdf"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2mUYWzURJL88",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import the required function\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "annak = ['Happy families are all alike;', 'every unhappy family is unhappy in its own way']\n",
        "\n",
        "# Call the vectorizer and fit it\n",
        "anna_vect = TfidfVectorizer().fit(annak)\n",
        "\n",
        "# Create the tfidf representation\n",
        "anna_tfidf = anna_vect.transform(annak)\n",
        "\n",
        "# Print the result \n",
        "print(anna_tfidf.toarray())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y7Df6or0UqYO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import the required vectorizer package and stop words list\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, ENGLISH_STOP_WORDS\n",
        "\n",
        "# Define the vectorizer and specify the arguments\n",
        "my_pattern = r'\\b[^\\d\\W][^\\d\\W]+\\b'\n",
        "vect = TfidfVectorizer(ngram_range=(1,2), max_features=100, token_pattern=my_pattern, stop_words=ENGLISH_STOP_WORDS).fit(tweets.text)\n",
        "\n",
        "# Transform the vectorizer\n",
        "X_txt = vect.transform(tweets.text)\n",
        " \n",
        "# Transform to a data frame and specify the column names\n",
        "X=pd.DataFrame(X_txt.toarray(), columns=vect.get_feature_names())\n",
        "print('Top 5 rows of the DataFrame: ', X.head())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kmA8KyxcUqgZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import the required packages\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "\n",
        "# Build a BOW and tfidf vectorizers from the review column and with max of 100 features\n",
        "vect1 = CountVectorizer(max_features=100).fit(reviews.review)\n",
        "vect2 = TfidfVectorizer(max_features=100).fit(reviews.review)\n",
        "\n",
        "# Transform the vectorizers\n",
        "X1 = vect1.transform(reviews.review)\n",
        "X2 = vect2.transform(reviews.review)\n",
        "# Create DataFrames from the vectorizers\n",
        "X_df1 = pd.DataFrame(X1.toarray(), columns=vect1.get_feature_names())\n",
        "X_df2 = pd.DataFrame(X2.toarray(), columns=vect2.get_feature_names())\n",
        "print('Top 5 rows, using BOW: \\n', X_df1.head())\n",
        "print('Top 5 rows using tfidf: \\n', X_df2.head())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fKR7t2uoUqof",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=123, stratify=y)\n",
        "\n",
        "# Train a logistic regression\n",
        "log_reg = LogisticRegression().fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_predicted = log_reg.predict(X_test)\n",
        "\n",
        "# Print the performance metrics\n",
        "print('Accuracy score test set: ', accuracy_score(y_test, y_predicted))\n",
        "print('Confusion matrix test set: \\n', confusion_matrix(y_test, y_predicted)/len(y_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xdv2a7clUqw4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import the accuracy and confusion matrix\n",
        "from sklearn.metrics import confusion_matrix,accuracy_score\n",
        "\n",
        "# Split the data into training and testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Build a logistic regression\n",
        "log_reg = LogisticRegression().fit(X_train,y_train)\n",
        "\n",
        "# Predict the labels \n",
        "y_predict = log_reg.predict(X_test)\n",
        "\n",
        "# Print the performance metrics\n",
        "print('Accuracy score of test data: ', accuracy_score(y_test, y_predict))\n",
        "print('Confusion matrix of test data: \\n', confusion_matrix(y_test, y_predict)/len(y_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OPkSctamUq_X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Split into training and testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=321)\n",
        "\n",
        "# Train a logistic regression\n",
        "log_reg = LogisticRegression().fit(X_train,y_train)\n",
        "\n",
        "# Predict the probability of the 0 class\n",
        "prob_0 = log_reg.predict_proba(X_test)[:, 0]\n",
        "# Predict the probability of the 1 class\n",
        "prob_1 = log_reg.predict_proba(X_test)[:, 1]\n",
        "\n",
        "print(\"First 10 predicted probabilities of class 0: \", prob_0[:10])\n",
        "print(\"First 10 predicted probabilities of class 1: \", prob_1[:10])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Rmb9gTwdDKf",
        "colab_type": "text"
      },
      "source": [
        "### Train a logistic regression with regularization parameter of 1000. Train a second logistic regression with regularization parameter equal to 0.001."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bMzJo1NbUrDe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Split data into training and testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
        "\n",
        "# Train a logistic regression with regularization of 1000\n",
        "log_reg1 = LogisticRegression(C=1000).fit(X_train, y_train)\n",
        "# Train a logistic regression with regularization of 0.001\n",
        "log_reg2 = LogisticRegression(C=0.001).fit(X_train, y_train)\n",
        "\n",
        "# Print the accuracies\n",
        "print('Accuracy of model 1: ', log_reg1.score(X_test, y_test))\n",
        "print('Accuracy of model 2: ', log_reg2.score(X_test, y_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UcFRiBtoUq8F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Build a logistic regression with regularizarion parameter of 100\n",
        "log_reg1 = LogisticRegression(C=100).fit(X_train,y_train)\n",
        "# Build a logistic regression with regularizarion parameter of 0.1\n",
        "log_reg2 = LogisticRegression(C=0.1).fit(X_train,y_train)\n",
        "\n",
        "# Predict the labels for each model\n",
        "y_predict1 = log_reg1.predict(X_test)\n",
        "y_predict2 = log_reg2.predict(X_test)\n",
        "\n",
        "# Print performance metrics for each model\n",
        "print('Accuracy of model 1: ', accuracy_score(y_test, y_predict1))\n",
        "print('Accuracy of model 2: ', accuracy_score(y_test, y_predict2))\n",
        "print('Confusion matrix of model 1: \\n' , confusion_matrix(y_test, y_predict1)/len(y_test))\n",
        "print('Confusion matrix of model 2: \\n', confusion_matrix(y_test, y_predict2)/len(y_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RvavNlU4d5v1",
        "colab_type": "text"
      },
      "source": [
        "# Bringing it all together\n",
        "\n",
        "## Step 1: Word cloud and feature creation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZixvVA2Uq5G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create and generate a word cloud image\n",
        "cloud_positives = WordCloud(background_color='white').generate(positive_reviews)\n",
        " \n",
        "# Display the generated wordcloud image\n",
        "plt.imshow(cloud_positives, interpolation='bilinear') \n",
        "plt.axis(\"off\")\n",
        "\n",
        "# Don't forget to show the final image\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_EZT4iQ6Uq3Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Tokenize each item in the review column\n",
        "word_tokens = [word_tokenize(review) for review in reviews.review]\n",
        "\n",
        "# Create an empty list to store the length of the reviews\n",
        "len_tokens = []\n",
        "\n",
        "# Iterate over the word_tokens list and determine the length of each item\n",
        "for i in range(len(word_tokens)):\n",
        "     len_tokens.append(len(word_tokens[i]))\n",
        "\n",
        "# Create a new feature for the lengh of each review\n",
        "reviews['n_words'] = len_tokens "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sauqhn1ufKo9",
        "colab_type": "text"
      },
      "source": [
        "## Step 2: Building a vectorizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o9pDpYKMUq1b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import the TfidfVectorizer and default list of English stop words\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, ENGLISH_STOP_WORDS\n",
        "\n",
        "# Build the vectorizer\n",
        "vect = TfidfVectorizer(stop_words=ENGLISH_STOP_WORDS, ngram_range=(1, 2), max_features=200, token_pattern=r'\\b[^\\d\\W][^\\d\\W]+\\b').fit(reviews.review)\n",
        "# Create sparse matrix from the vectorizer\n",
        "X = vect.transform(reviews.review)\n",
        "\n",
        "# Create a DataFrame\n",
        "reviews_transformed = pd.DataFrame(X.toarray(), columns=vect.get_feature_names())\n",
        "print('Top 5 rows of the DataFrame: \\n', reviews_transformed.head())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Fz8_CnDfHE7",
        "colab_type": "text"
      },
      "source": [
        "## Step 3: Building a classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "do-dJ3qYUquM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define X and y\n",
        "y = reviews_transformed.score\n",
        "X = reviews_transformed.drop('score', axis=1)\n",
        "\n",
        "# Train/test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=456)\n",
        "\n",
        "# Train a logistic regression\n",
        "log_reg = LogisticRegression().fit(X_train, y_train)\n",
        "# Predict the labels\n",
        "y_predicted = log_reg.predict(X_test)\n",
        "\n",
        "# Print accuracy score and confusion matrix on test set\n",
        "print('Accuracy on the test set: ', accuracy_score(y_test, y_predicted))\n",
        "print(confusion_matrix(y_test, y_predicted)/len(y_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UdSQsuDKUqr-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}